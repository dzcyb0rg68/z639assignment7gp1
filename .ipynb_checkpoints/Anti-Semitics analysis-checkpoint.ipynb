{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /u/cc93/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import lzma\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = open('antisemitics.txt', 'w+') #hulu is ranked 3956\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t %s\\t %s\\t %s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'subreddit' ,'author', 'date', 'socre' ,'word', 'datetime', 'distict_score', 'shared_word','comment'))\n",
    "head.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "antisemitics_words = ['libel', 'clannish', 'conspiracy', 'cowardice', 'goyim', 'globalist', 'greed', 'holocaust', 'jew', 'jewish', 'illuminati', 'khazars', 'kosher', 'zionish', 'scapegoat', 'silencing', 'smirking', 'merchant']\n",
    "antisemitics_set = set(antisemitics_words)\n",
    "\n",
    "filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\"]\n",
    "i = 0\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                words = re.findall(r'\\w+', line['body'].lower())\n",
    "                words_set = set(words)\n",
    "                common_elements = words_set.intersection(antisemitics_set)\n",
    "                score = len(common_elements)\n",
    "                antisemitics_list = []\n",
    "                if score >= 1:\n",
    "                    for w in words:\n",
    "                        for a in antisemitics_words:\n",
    "                            if w == a:\n",
    "                                antisemitics_list.append(w)\n",
    "                                \n",
    "                    ndate = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=(line['created_utc']))\n",
    "                    ndate = ndate.isoformat().replace('T',' ')\n",
    "                    file = open('antisemitics.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %s\\t %s\\t %d\\t %s\\t %s\\t %d\\t %s\\t %s\\t\\n'% (i+1, line['subreddit'], line['author'].replace('\\n',''), date[-7:], len(antisemitics_list), antisemitics_list, ndate, score, list(dict.fromkeys(antisemitics_list)), line['body'].replace('\\n','')))\n",
    "                    i += 1\n",
    "                    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-01-01 00:05:38'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndate = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=(1514765138))\n",
    "ndate.isoformat().replace('T',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n"
     ]
    }
   ],
   "source": [
    "Waasd = ['jewish', 'jewish', 'jewish']\n",
    "aa = list(dict.fromkeys(Waasd))\n",
    "aa\n",
    "\n",
    "common_elements = set(Waasd).intersection(antisemitics_set)\n",
    "score = len(common_elements)\n",
    "print(score, len(Waasd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2c0fd7744c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'xz'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlzma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mredditfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mredditfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "antisemitics_words = ['libel', 'clannish', 'conspiracy', 'cowardice', 'goyim', 'globalist', 'greed', 'holocaust', 'jew', 'jewish', 'illuminati', 'khazars', 'kosher', 'zionish', 'scapegoat', 'silencing', 'smirking', 'merchant']\n",
    "antisemitics_set = set(antisemitics_words)\n",
    "\n",
    "filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                words = re.findall(r'\\w+', line['body'].lower())\n",
    "                words_set = set(words)\n",
    "                common_elements = words_set.intersection(antisemitics_set)\n",
    "                score = len(common_elements)\n",
    "                antisemitics_list = []\n",
    "                if score >= 1:\n",
    "                    for w in words:\n",
    "                        for a in antisemitics_words:\n",
    "                            if w == a:\n",
    "                                antisemitics_list.append(w)\n",
    "                                \n",
    "                    ndate = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=(line['created_utc']))\n",
    "                    ndate = ndate.isoformat().replace('T',' ')\n",
    "                    line['date_year_month'] = date[-7:]\n",
    "                    line['created_utc_converted'] = ndate\n",
    "                    line['score_overall'] = len(antisemitics_list)\n",
    "                    line['words'] = antisemitics_list\n",
    "                    line['shared_words'] = list(dict.fromkeys(antisemitics_list))\n",
    "                    line['socre_distint'] = score\n",
    "                    with open('antisemitics.json', 'a') as outfile:\n",
    "                        json.dump(line, outfile)\n",
    "                        outfile.write('\\n')\n",
    "                        outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
