{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import Counter\n",
    "import sys\n",
    "import re\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_2016():\n",
    "    count = 0\n",
    "    filenames = [\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-01.gz\",\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-02.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-05.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-07.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-09.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-11.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-03.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-04.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-06.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-08.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-10.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-12.gz\"\n",
    "    ]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_june_2019():\n",
    "    count = 0\n",
    "\n",
    "    filenames = [\"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2019-06.gz\"]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = most_active_subreddit_in_2016()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(c, key=c.get, reverse=True)\n",
    "sorted(c.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count,dd = most_active_subreddit_in_june_2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_subreddit = sorted(count, key=count.get, reverse=True)\n",
    "sorted_subreddit_tuple = sorted(count.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comment_rank_subreddit-1-persent_June2019.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s %s' % x for x in sorted_subreddit_tuple))\n",
    "    \n",
    "with open('comment_rank_subreddit-1-persent_June2019.txt') as fp2:\n",
    "    for i, line, x in zip(range(len(sorted_subreddit)),enumerate(fp2),sorted_subreddit):\n",
    "#         print(i+1, x, count[x])\n",
    "        with open('rank.txt','a') as fp3:\n",
    "            fp3.write('%d %s %s %s'%(i+1, x, count[x], 'https://www.reddit.com/r/'+x+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "# import gzip\n",
    "import lzma\n",
    "\n",
    "filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-02.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-03.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-04.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-05.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-06.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-07.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-08.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-09.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-10.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-11.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-12.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-01.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-02.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-03.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-04.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-05.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-06.zst\"\n",
    "            ]\n",
    "\n",
    "head = open('hulu.txt', 'w+') #hulu is ranked 3956\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'score', 'date', 'comment'))\n",
    "head.close()\n",
    "\n",
    "head = open('netflix.txt', 'w+') #netflix is ranked 1642\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'score', 'date', 'comment'))\n",
    "head.close()\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "#     with gzip.open(filename) as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                if line['subreddit'] == 'Hulu':    \n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (i+1, line['author'].replace('\\n',''), line['score'] , date[-7:], line['body'].replace('\\n','')))\n",
    "                    i += 1\n",
    "                    file.close()\n",
    "                    \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (j+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    j += 1\n",
    "                    file.close()\n",
    "                    \n",
    "    elif filename.split('.',1)[1] == 'zst':\n",
    "        with open(filename, 'rb') as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            stream_reader = dctx.stream_reader(fh)\n",
    "            text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "            z = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "            \n",
    "            for line in text_stream:\n",
    "                line = json.loads(line.translate(z))\n",
    "                if line['subreddit'] == 'Hulu':\n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (i+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    i += 1\n",
    "                    file.close()\n",
    "                \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %d\\t %s\\t %s\\t\\n'% (j+1, line['author'].replace('\\n',''), line['score'], date[-7:], line['body'].replace('\\n','')))\n",
    "                    j += 1\n",
    "                    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = {'all_awardings': [],\n",
    "#  'author': 'ragenukem',\n",
    " 'author_created_utc': 1343369985,\n",
    " 'author_flair_background_color': None,\n",
    " 'author_flair_css_class': None,\n",
    " 'author_flair_richtext': [],\n",
    " 'author_flair_template_id': None,\n",
    " 'author_flair_text': None,\n",
    " 'author_flair_text_color': None,\n",
    " 'author_flair_type': 'text',\n",
    " 'author_fullname': 't2_8gva8',\n",
    " 'author_patreon_flair': False,\n",
    "#  'body': 'I get to do the thing! /r/beetlejuicing',\n",
    " 'can_gild': True,\n",
    " 'can_mod_post': False,\n",
    " 'collapsed': False,\n",
    " 'collapsed_reason': None,\n",
    " 'controversiality': 0,\n",
    " 'created_utc': 1559347203,\n",
    " 'distinguished': None,\n",
    " 'edited': False,\n",
    " 'gilded': 0,\n",
    " 'gildings': {},\n",
    " 'id': 'epom0fx',\n",
    " 'is_submitter': False,\n",
    " 'link_id': 't3_bv9v53',\n",
    " 'locked': False,\n",
    " 'no_follow': True,\n",
    " 'parent_id': 't1_epobwr9',\n",
    " 'permalink': '/r/Wellthatsucks/comments/bv9v53/ball_boy_meet_wall_boy/epom0fx/',\n",
    " 'quarantined': False,\n",
    " 'removal_reason': None,\n",
    " 'retrieved_on': 1568677948,\n",
    "#  'score': 2,\n",
    "#  'send_replies': True,\n",
    " 'steward_reports': [],\n",
    " 'stickied': False,\n",
    "#  'subreddit': 'Wellthatsucks',\n",
    " 'subreddit_id': 't5_2xcv7',\n",
    " 'subreddit_name_prefixed': 'r/Wellthatsucks',\n",
    " 'subreddit_type': 'public',\n",
    " 'total_awards_received': 0}\n",
    "\n",
    "def remove_key(target_dict, source_dict):\n",
    "    for key in source_dict:\n",
    "        try:\n",
    "            del target_dict[key]\n",
    "        except KeyError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def convert_sectodate(sec):\n",
    "    ndate = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=(sec))\n",
    "    return ndate.isoformat().replace('T',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "# import gzip\n",
    "import lzma\n",
    "\n",
    "filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-02.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-03.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-04.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-05.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-06.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-07.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-08.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-09.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-10.xz\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-11.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2018-12.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-01.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-02.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-03.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-04.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-05.zst\",\n",
    "    \"/l/research/social-media-mining/reddit/comments/RC_2019-06.zst\"\n",
    "            ]\n",
    "\n",
    "i = 1\n",
    "j = 1\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "#     with gzip.open(filename) as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                if line['subreddit'] == 'Hulu':    \n",
    "                    ndate = convert_sectodate(line['created_utc'])\n",
    "                    line['date_year_month'] = date[-7:]\n",
    "                    line['created_utc_converted'] = ndate\n",
    "                    line['row_number'] = i\n",
    "                    remove_key(line, sample_dict)\n",
    "                    with open('Hulu.json', 'a') as Hulu:\n",
    "                        json.dump(line, Hulu)\n",
    "                        Hulu.write('\\n')\n",
    "                        Hulu.close()\n",
    "                    i += 1\n",
    "                    \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    ndate = convert_sectodate(line['created_utc'])\n",
    "                    line['date_year_month'] = date[-7:]\n",
    "                    line['created_utc_converted'] = ndate\n",
    "                    line['row_number'] = j\n",
    "                    remove_key(line, sample_dict)\n",
    "                    with open('netflix.json', 'a') as netflix:\n",
    "                        json.dump(line, netflix)\n",
    "                        netflix.write('\\n')\n",
    "                        netflix.close()\n",
    "                    j += 1\n",
    "                    \n",
    "    elif filename.split('.',1)[1] == 'zst':\n",
    "        with open(filename, 'rb') as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            stream_reader = dctx.stream_reader(fh)\n",
    "            text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "            z = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "            \n",
    "            for line in text_stream:\n",
    "                line = json.loads(line.translate(z))\n",
    "                if line['subreddit'] == 'Hulu':\n",
    "                    ndate = convert_sectodate(line['created_utc'])\n",
    "                    line['date_year_month'] = date[-7:]\n",
    "                    line['created_utc_converted'] = ndate\n",
    "                    line['row_number'] = i\n",
    "                    remove_key(line, sample_dict)\n",
    "                    with open('Hulu.json', 'a') as Hulu:\n",
    "                        json.dump(line, Hulu)\n",
    "                        Hulu.write('\\n')\n",
    "                        Hulu.close()\n",
    "                    i += 1\n",
    "                \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    ndate = convert_sectodate(line['created_utc'])\n",
    "                    line['date_year_month'] = date[-7:]\n",
    "                    line['created_utc_converted'] = ndate\n",
    "                    line['row_number'] = j\n",
    "                    remove_key(line, sample_dict)\n",
    "                    with open('netflix.json', 'a') as netflix:\n",
    "                        json.dump(line, netflix)\n",
    "                        netflix.write('\\n')\n",
    "                        netflix.close()\n",
    "                    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
