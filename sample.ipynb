{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import Counter\n",
    "import sys\n",
    "import re\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_2016():\n",
    "    count = 0\n",
    "    filenames = [\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-01.gz\",\n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-02.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-05.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-07.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-09.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-11.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-03.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-04.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-06.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-08.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-10.gz\", \n",
    "    \"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2016-12.gz\"\n",
    "    ]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_subreddit_in_june_2019():\n",
    "    count = 0\n",
    "\n",
    "    filenames = [\"/l/research/social-media-mining/reddit-sample-1-percent/comments/RC_2019-06.gz\"]\n",
    "    \n",
    "    l = []\n",
    "    for filename in filenames:\n",
    "        for line in gzip.open(filename):\n",
    "            line = line.decode(\"utf-8\")\n",
    "            \n",
    "            line = json.loads(line)\n",
    "            l.append(line['subreddit'])\n",
    "    \n",
    "    c = Counter(l)\n",
    "    return (c,c.most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,d = most_active_subreddit_in_2016()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(c, key=c.get, reverse=True)\n",
    "sorted(c.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count,dd = most_active_subreddit_in_june_2019()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_subreddit = sorted(count, key=count.get, reverse=True)\n",
    "sorted_subreddit_tuple = sorted(count.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comment_rank_subreddit-1-persent_June2019.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s %s' % x for x in sorted_subreddit_tuple))\n",
    "    \n",
    "with open('comment_rank_subreddit-1-persent_June2019.txt') as fp2:\n",
    "    for i, line, x in zip(range(len(sorted_subreddit)),enumerate(fp2),sorted_subreddit):\n",
    "#         print(i+1, x, count[x])\n",
    "        with open('rank.txt','a') as fp3:\n",
    "            fp3.write('%d %s %s %s'%(i+1, x, count[x], 'https://www.reddit.com/r/'+x+'\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e920b05330c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'netflix.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#netflix is ranked 1642\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\t %s\\t %s\\t %s\\t\\n'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'row number'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import io\n",
    "import zstandard as zstd\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# filenames = [\"/l/research/social-media-mining/reddit/comments/RC_2018-01.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-02.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-03.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-04.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-05.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-06.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-07.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-08.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-09.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-10.xz\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-11.zst\",\n",
    "#     \"/l/research/social-media-mining/reddit/comments/RC_2018-12.zst\"\n",
    "#             ]\n",
    "\n",
    "head = open('hulu.txt', 'w+') #hulu is ranked 3956\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'comment', 'date'))\n",
    "head.close()\n",
    "\n",
    "head = open('netflix.txt', 'w+') #netflix is ranked 1642\n",
    "head.write('%s\\t %s\\t %s\\t %s\\t\\n'% ('row number', 'author', 'comment', 'date'))\n",
    "head.close()\n",
    "\n",
    "comments = []\n",
    "authors = []\n",
    "i = 0\n",
    "j = 0\n",
    "k = 0\n",
    "l = 0\n",
    "for filename in filenames:\n",
    "    date = filename.replace('.xz','').replace('.zst','')\n",
    "    if filename.split('.',1)[1] == 'xz':\n",
    "        with lzma.open(filename, mode='rt', encoding='utf-8') as redditfile:\n",
    "#     with gzip.open(filename) as redditfile:\n",
    "            for line in redditfile:\n",
    "                line = json.loads(line)\n",
    "                if line['subreddit'] == 'hulu':    \n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %s\\t %s\\t\\n'% (i+1, line['author'].replace('\\n',''), line['body'].replace('\\n',''), date[-7:]))\n",
    "                    i += 1\n",
    "                    file.close()\n",
    "                    \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %s\\t %s\\t\\n'% (j+1, line['author'].replace('\\n',''), line['body'].replace('\\n',''), date[-7:]))\n",
    "                    j += 1\n",
    "                    file.close()\n",
    "                    \n",
    "    elif filename.split('.',1)[1] == 'zst':\n",
    "        with open(filename, 'rb') as fh:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            stream_reader = dctx.stream_reader(fh)\n",
    "            text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "            z = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)\n",
    "            \n",
    "            for line in text_stream:\n",
    "                line = json.loads(line.translate(z))\n",
    "                if line['subreddit'] == 'hulu':\n",
    "                    file = open('hulu.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %s\\t %s\\t\\n'% (k+1, line['author'].replace('\\n',''), line['body'].replace('\\n',''), date[-7:]))\n",
    "                    k += 1\n",
    "                    file.close()\n",
    "                \n",
    "                if line['subreddit'] == 'netflix':    \n",
    "                    file = open('netflix.txt', 'a+')\n",
    "                    file.write('%d\\t %s\\t %s\\t %s\\t\\n'% (l+1, line['author'].replace('\\n',''), line['body'].replace('\\n',''), date[-7:]))\n",
    "                    l += 1\n",
    "                    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
